---
name: review-code
description: Perform a critical code review of all changes on the current branch compared to main
disable-model-invocation: true
allowed-tools: Bash, Read, Grep, Glob
---

You are performing a rigorous code review of all changes on the current branch compared to main. Your job is to find problems, not to be agreeable.

## Step 1: Gather Context

Run these commands to understand the full scope of changes:

```
git fetch origin main
git diff origin/main...HEAD
git log origin/main..HEAD --oneline
git diff origin/main...HEAD --stat
```

Read every changed file in full (not just the diff) to understand the surrounding context.

Also read the project's `CLAUDE.md` (if it exists) to understand project-specific coding standards, conventions, and quality expectations. Apply those standards throughout the review.

## Step 2: Review Mindset

- **Default to skepticism.** Assume there are problems until you've proven otherwise.
- **Read every line of the diff.** Don't skim. Understand what each change does and why.
- **Review what's NOT there.** Missing error handling, missing tests, missing edge cases, and missing documentation are all defects.
- **Question the premise.** Before reviewing the implementation, ask: is this the right thing to build? Does it actually solve the stated problem? Could the problem be solved without code changes at all?

## Step 3: Feature Fitness

- **Does this solve the actual problem?** Restate the problem in your own words based on the branch name, commit messages, and code changes. Then check if the implementation addresses it. Flag if the solution solves a different or broader problem than what was asked for.
- **Is anything unnecessary?** Flag abstractions, configurability, extensibility points, or helper methods that serve no current requirement. Every line of code is a liability.
- **Would a simpler approach work?** If a 5-line change could replace a 50-line change, say so. Propose the simpler alternative concretely.
- **Is this the minimal change?** Check for scope creep: wholesale refactoring of unrelated code, added features that weren't requested, or large structural changes unrelated to the goal. However, **small, clear improvements to files already being touched are encouraged** â€” see the "Leave It Better" section below.

## Step 4: Complexity and Maintenance Burden

For every change, explicitly assess:

- **Does this increase, decrease, or maintain the current complexity?** State which and why.
- **New concepts introduced:** Does this add new patterns, abstractions, services, or conventions that the team must now understand and maintain?
- **Dependency impact:** Does this add new dependencies, or increase coupling between existing components?
- **Future maintenance cost:** Will this require ongoing attention? Does it create something that can silently break or drift out of sync?
- **Readability:** Could a developer unfamiliar with this code understand it without explanation? If not, the code is too clever.

## Step 5: Leave It Better

When touching a file, the author should leave it better than they found it. Check whether the changed files have nearby opportunities for clear, concise improvements. These are **encouraged** and should be called out if missing:

- **Explicit types.** Are variable types, return types, or parameter types implicit where they could be explicit?
- **Dead code.** Are there unused imports, unreachable branches, commented-out code, or unused variables in the changed files?
- **Style consistency.** Does the changed code match the surrounding style? Are there inconsistent naming conventions, spacing, or formatting in the same file?
- **Clarifying comments.** Is there non-obvious logic that would benefit from a brief comment? Conversely, are there stale or misleading comments that should be updated or removed?
- **Naming.** Are variable, method, or class names clear and accurate? A rename in a touched file is a welcome improvement.
- **Performance.** Correctness and readability come first, but flag obvious performance issues: unnecessary allocations in hot paths, O(nÂ²) when O(n) is straightforward, repeated expensive operations that could be cached, missing pagination on unbounded queries, or blocking calls where async is expected. Don't suggest micro-optimizations that hurt readability.
- **Logging.** Use Grep to find the logging pattern used elsewhere in the codebase (e.g., `logger`, `console.log`, `Log.`, `logging.`, `slog.`, etc.). Then check: does the changed code log appropriately? Are error paths logged? Are key operations traceable? Does the log level match the codebase conventions (debug vs info vs warn vs error)? If the surrounding codebase has logging but the changed code does not, flag it. Even if the project has no logging at all, suggest adding it when the changed code touches precarious areas: payment/billing, authentication, data mutations, external API calls, scheduled jobs, or anything where silent failure would be costly to debug in production. Flag any caught exceptions that are swallowed silently (empty catch blocks, catch-and-continue with no logging or re-throw) â€” these hide bugs and make production issues nearly impossible to diagnose.

**Boundaries:** These improvements should be limited to files already being modified and should be clear, concise, and obviously correct. Wholesale refactoring of unrelated code, large structural changes, or changes that require extensive testing are not in scope here.

If the author made good opportunistic improvements, acknowledge them. If obvious opportunities were missed in touched files, list them under "Improvement Opportunities" in the output.

## Step 6: Unintended Consequences

- **Trace all callers and consumers.** If a method signature, return type, or behavior changed, verify every call site still works correctly. Use Grep to find all references.
- **Check for state mutations.** Does this change shared state, static fields, singleton behavior, or cached data in ways that affect other code paths?
- **Consider timing and ordering.** Does this change when something executes? Could it create race conditions, deadlocks, or ordering dependencies?
- **Check boundary conditions.** What happens with null inputs, empty collections, first run, network failure, concurrent access, or maximum data volumes?
- **Platform impact.** If the change touches shared code, verify it works correctly on both Android and iOS. If it touches platform-specific code, verify the other platform's equivalent is still consistent.

## Step 7: Assumptions Audit

List every assumption the code makes. Then verify each one:

- Does this assume data will always be in a certain format?
- Does this assume a service will always be available?
- Does this assume a certain execution order?
- Does this assume non-null values without checking?
- Does this assume collection sizes or string lengths?
- Does this assume API responses won't change?

Flag assumptions that weren't validated. If the code assumes something that could reasonably be false, it needs either validation or a comment explaining why the assumption is safe.

## Step 8: Test Coverage

- **Are there tests?** If not, why not? "It's hard to test" is not an acceptable reason â€” it usually means the code needs restructuring.
- **Do tests cover the actual behavior change?** Tests that pass regardless of whether the feature works are worthless.
- **Are edge cases tested?** Null inputs, empty data, error conditions, boundary values, concurrent scenarios.
- **Are negative tests included?** Tests that verify the code correctly rejects invalid input or handles failure gracefully.
- **Do existing tests still pass?** A change that breaks existing tests is a red flag that the author may not understand the system's contracts.
- **Is the test testing implementation or behavior?** Tests coupled to implementation details are brittle. Tests should verify observable behavior.

## Step 9: Output

Structure your review as follows. Use the emoji prefixes exactly as shown â€” they provide visual severity scanning.

---

### Verdict

Use one of:
- **APPROVE** â€” no blocking issues
- **REQUEST CHANGES** â€” has critical issues that must be fixed
- **NEEDS DISCUSSION** â€” has open questions that need alignment

### Summary

One paragraph restating what this change does and whether it achieves its goal.

**Complexity:** Increases / Decreases / Neutral â€” with brief justification.

### Findings

List every finding as a single flat list. Each finding is one line with a severity emoji prefix, the file path and line number, and a concise description:

- Use these severity levels:
  - `ðŸ”´` **Critical** â€” must fix before merge (correctness, security, data loss)
  - `ðŸŸ¡` **Warning** â€” should fix, potential for bugs or maintenance issues
  - `ðŸ’¡` **Suggestion** â€” optional improvement (style, naming, cleanup, simplification)
  - `âœ…` **Good** â€” something done well worth calling out (use sparingly)

- Format each finding as:
  > `ðŸ”´ path/to/file.ts:42 â€” Description of the issue`

- Group findings by file when multiple findings affect the same file.
- Include findings from ALL review steps: unintended consequences, assumptions, missing tests, improvement opportunities, simplification, and minor issues.
- If there are no findings at all, say "No issues found."

### Test Gaps

If there are specific untested scenarios, list them as a brief checklist:

- `â¬œ Scenario that needs a test`

If coverage is adequate, say "Coverage is adequate."

### Bottom Line

A 2-3 sentence executive summary. State the verdict again, the number of critical/warning/suggestion findings, and the single most important thing the author should address. This is what someone skimming reads first after the verdict.
